from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import csv

options = Options()
options.add_argument("--start-maximized")
service = Service()
driver = webdriver.Chrome(service=service, options=options)
wait = WebDriverWait(driver, 10)

# ì—…ì¢… ë¦¬ìŠ¤íŠ¸
categories = ["í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "íŒ¨ìŠ¤íŠ¸í‘¸ë“œ", "ì–‘ì‹", "ë¶„ì‹"]

all_data = []

def crawl_category(category):
    try:
        keyword = f"ì²­ì£¼ {category} ì‹ë‹¹"
        print(f"\nğŸ” '{keyword}' ê²€ìƒ‰ ì‹œì‘")
        driver.get("https://map.kakao.com/")

        search_box = wait.until(EC.presence_of_element_located((By.ID, "search.keyword.query")))
        search_box.clear()
        search_box.send_keys(keyword)
        time.sleep(1)

        try:
            dimmed = driver.find_element(By.ID, "dimmedLayer")
            if dimmed.is_displayed():
                driver.execute_script("document.getElementById('dimmedLayer').remove()")
                time.sleep(0.5)
        except:
            pass

        submit_btn = driver.find_element(By.ID, "search.keyword.submit")
        submit_btn.click()

        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".placelist")))

        page = 1
        while True:
            time.sleep(1.5)
            items = driver.find_elements(By.CSS_SELECTOR, ".placelist .PlaceItem")
            print(f"ğŸ“„ {category} - {page}í˜ì´ì§€: {len(items)}ê°œ")

            for item in items:
                try:
                    name = item.find_element(By.CSS_SELECTOR, "a.link_name").text.strip()
                    category_text = item.find_element(By.CSS_SELECTOR, "span.subcategory.clickable").text.strip()
                    address = item.find_element(By.CSS_SELECTOR, "p[data-id='address']").text.strip()
                    print(f"ìˆ˜ì§‘: {name} | {category_text} | {address}")
                    all_data.append([name, address, category_text, category])
                except Exception as e:
                    print(f"ì•„ì´í…œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    continue

            try:
                next_btn = driver.find_element(By.CSS_SELECTOR, "a.btn_next")
                if "off" in next_btn.get_attribute("class"):
                    break
                next_btn.click()
                page += 1
            except:
                break

        print(f"âœ… '{category}' ì™„ë£Œ (ì´ {page}í˜ì´ì§€)")
    except Exception as e:
        print(f"âŒ '{category}' ì—ëŸ¬ ë°œìƒ: {e}")

for category in categories:
    crawl_category(category)

print(f"\nì´ ìˆ˜ì§‘ ë°ì´í„° ìˆ˜: {len(all_data)}")

# ì¤‘ë³µ ì œê±°
unique_data = []
seen = set()
for row in all_data:
    key = (row[0], row[1])  # ì´ë¦„+ì£¼ì†Œ ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    if key not in seen:
        seen.add(key)
        unique_data.append(row)

print(f"ì¤‘ë³µ ì œê±° í›„ ë°ì´í„° ìˆ˜: {len(unique_data)}")

with open("cheongju_category_restaurants.csv", "w", newline="", encoding="utf-8-sig") as f:
    writer = csv.writer(f)
    writer.writerow(["name", "address", "category_text", "searched_category"])
    writer.writerows(unique_data)

print(f"\nğŸ’¾ ì™„ë£Œ: ì´ {len(unique_data)}ê°œ ì‹ë‹¹ ì €ì¥ë¨ (cheongju_category_restaurants.csv)")

driver.quit()
